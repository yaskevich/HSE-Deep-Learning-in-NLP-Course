{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\dev\\\\HSE-Deep-Learning-in-NLP-Course\\\\week_02'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data\\corpus.json', encoding=\"utf-8\") as f:\n",
    "    corpus = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Текстов - 1157366, слов - 16028874'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Текстов - {}, слов - {}'.format(len(corpus), sum([len(sample) for sample in corpus]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lemma(word):\n",
    "    \n",
    "    word_data = morph.parse(word)[0]\n",
    "    \n",
    "    return word_data.normal_form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Соберем словарь лем\n",
    "В нашем корпусе 16028874 слов. Лемматизировать весь корпус будет очень долго. Давайте лучше соберем словарь уникальных слов и будет лемматизировать только уникальные слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 1157366/1157366 [01:29<00:00, 12873.91it/s]\n"
     ]
    }
   ],
   "source": [
    "tok2lemma = {}\n",
    "\n",
    "for text in tqdm(corpus):\n",
    "    for tok in text:\n",
    "        if tok not in tok2lemma:\n",
    "            tok2lemma[tok] = get_lemma(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'уехать'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ключ - уникальное слово в нашем корпусе, значение - его лемма\n",
    "tok2lemma['уехали']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "193435"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tok2lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('russian')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Замена слов леммами\n",
    "Так как теперь к каждому слову из нашего корпуса мы знаем лемму, то давайте каждое слово заменим на его лемму и уберем стоп слова.\n",
    "Это работает гораздо(!) быстрее, чем если бы мы в корпусе для каждого слова каждый раз рассчитывали лемму (с помощью пайморфи), \n",
    "потому что теперь нам надо вызвать пайморфи 193435 раз вместо 16028874."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 1157366/1157366 [00:52<00:00, 21908.18it/s]\n"
     ]
    }
   ],
   "source": [
    "lemmas_corpus = [[tok2lemma[tok] for tok in text if tok not in stopwords and tok]\n",
    "                 for text in tqdm(corpus)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Соберем частотный словарь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 1157366/1157366 [00:07<00:00, 144861.66it/s]\n"
     ]
    }
   ],
   "source": [
    "freq = {}\n",
    "\n",
    "for text in tqdm(lemmas_corpus):\n",
    "    for tok in text:\n",
    "        if tok in freq:\n",
    "            freq[tok] += 1\n",
    "        else:\n",
    "            freq[tok] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_df = pd.DataFrame(data={'word': list(freq.keys()), 'n_entries': list(freq.values())})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_df.sort_values(by=['n_entries'], ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>n_entries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>##число</td>\n",
       "      <td>413016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>банка</td>\n",
       "      <td>197884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>карта</td>\n",
       "      <td>156216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>банк</td>\n",
       "      <td>135943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>кредит</td>\n",
       "      <td>86865</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        word  n_entries\n",
       "21   ##число     413016\n",
       "3      банка     197884\n",
       "47     карта     156216\n",
       "25      банк     135943\n",
       "134   кредит      86865"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77002, 2)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# уникальных слов в словаре\n",
    "freq_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>n_entries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>45645</th>\n",
       "      <td>пролом</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45647</th>\n",
       "      <td>запачкать</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45648</th>\n",
       "      <td>гих</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45650</th>\n",
       "      <td>поcтоянно</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77001</th>\n",
       "      <td>ситуевина</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            word  n_entries\n",
       "45645     пролом          1\n",
       "45647  запачкать          1\n",
       "45648        гих          1\n",
       "45650  поcтоянно          1\n",
       "77001  ситуевина          1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_words = freq_df.n_entries.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Замена редких слов\n",
    "В нашем корпусе осталось много слов, которые встречаются очень редко. Давайте мы редкие слова заменим на специальный токе UNK - unknown. Так мы разительно сократим размер нашего словаря слов с незначительной потерей информации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Доля слов, которые мы заменим на UNK:\n",
      "Порог отсечения - 5, доля UNK - 0.76 %, слов в слове - 23389, удалили - 53613 слов\n",
      "Порог отсечения - 10, доля UNK - 1.18 %, слов в слове - 16851, удалили - 60151 слов\n",
      "Порог отсечения - 15, доля UNK - 1.51 %, слов в слове - 13861, удалили - 63141 слов\n",
      "Порог отсечения - 20, доля UNK - 1.82 %, слов в слове - 12010, удалили - 64992 слов\n",
      "Порог отсечения - 25, доля UNK - 2.07 %, слов в слове - 10798, удалили - 66204 слов\n",
      "Порог отсечения - 30, доля UNK - 2.31 %, слов в слове - 9882, удалили - 67120 слов\n",
      "Порог отсечения - 35, доля UNK - 2.53 %, слов в слове - 9170, удалили - 67832 слов\n"
     ]
    }
   ],
   "source": [
    "print('Доля слов, которые мы заменим на UNK:')\n",
    "\n",
    "for threshold in np.arange(5, 36, 5):\n",
    "    \n",
    "    sub_df = freq_df[freq_df.n_entries < threshold]\n",
    "    \n",
    "    unk_freq = sub_df['n_entries'].sum() * 100 / n_words\n",
    "    \n",
    "    print('Порог отсечения - {}, доля UNK - {:.2f} %, слов в слове - {}, удалили - {} слов'.format(\n",
    "        threshold, unk_freq, freq_df.shape[0] - sub_df.shape[0], sub_df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# кажется, что оптимально, но обычно берут меньше\n",
    "threshold = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = freq_df[freq_df.n_entries >= threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = set(vocab.word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13861"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Мы сократили наш словарь в 5.56 раз с потерей 1.51 % всех слов'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Мы сократили наш словарь в {:.2f} раз с потерей 1.51 % всех слов'.format(freq_df.shape[0] / len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correct_words(word):\n",
    "    \n",
    "    if word in words:\n",
    "        return word\n",
    "    else:\n",
    "        return 'UNK'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 1157366/1157366 [00:07<00:00, 164156.01it/s]\n"
     ]
    }
   ],
   "source": [
    "# заменим слово токеном UNK, если его нет в нашем новом словаре\n",
    "processed_corpus = [[get_correct_words(tok) for tok in text] for text in tqdm(lemmas_corpus)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_duplicate_unks(tokens):\n",
    "    \n",
    "    output_tokens = []\n",
    "    \n",
    "    for tok in tokens:\n",
    "        \n",
    "        if tok == 'UNK' and output_tokens and output_tokens[-1] == 'UNK':\n",
    "            continue\n",
    "            \n",
    "        output_tokens.append(tok)\n",
    "            \n",
    "    return output_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = 'думать далее милый барышня UNK UNK тинькоф звонить неделя'.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['думать',\n",
       " 'далее',\n",
       " 'милый',\n",
       " 'барышня',\n",
       " 'UNK',\n",
       " 'UNK',\n",
       " 'тинькоф',\n",
       " 'звонить',\n",
       " 'неделя']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['думать', 'далее', 'милый', 'барышня', 'UNK', 'тинькоф', 'звонить', 'неделя']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_duplicate_unks(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 1157366/1157366 [00:06<00:00, 170466.90it/s]\n"
     ]
    }
   ],
   "source": [
    "# дедублируем подряд идущие унки (оставим только один)\n",
    "processed_corpus = [drop_duplicate_unks(sample) for sample in tqdm(processed_corpus)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Текстов с унками - 11.28 %'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_with_unk = [text for text in processed_corpus if 'UNK' in text]\n",
    "'Текстов с унками - {:.2f} %'.format(len(texts_with_unk) * 100 / len(processed_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "оплата покупка UNK интернет магазин номер карта срок действие шок\n",
      "ответ самый разнообразный UNK наверное люба документ подтверждать родственный связь\n",
      "сотрудник колл центр считать UNK звонить суббота ##число утро вечером том плата\n",
      "благодарность адресовать UNK вячеслав главное менеджер отдел продажа ипотечный кредит\n",
      "##число год случиться кризис семейный бизнес стать трещать UNK\n"
     ]
    }
   ],
   "source": [
    "# посмотрим на тексты с унками\n",
    "for text in random.sample(texts_with_unk, k=5):\n",
    "    print(' '.join(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# выглядит не так плохо"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(processed_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Выберем подвыборку данных\n",
    "Чтобы быстрее выучить word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_data = processed_corpus[:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/processed_corpus.json', 'w', encoding=\"utf-8\") as f:\n",
    "    json.dump(sub_data, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
